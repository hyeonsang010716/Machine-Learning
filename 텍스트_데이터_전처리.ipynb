{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMsCuWREi6qiiXcEou3Z+8p",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hyeonsang010716/Machine_Learning/blob/main/%ED%85%8D%EC%8A%A4%ED%8A%B8_%EB%8D%B0%EC%9D%B4%ED%84%B0_%EC%A0%84%EC%B2%98%EB%A6%AC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download(\"book\", quiet=True)\n",
        "from nltk.book import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "Z4SCerEP7E8W",
        "outputId": "f67d93f2-c46d-4661-cf83-f0f5a3ea1c98"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "*** Introductory Examples for the NLTK Book ***\n",
            "Loading text1, ..., text9 and sent1, ..., sent9\n",
            "Type the name of the text or sentence to view it.\n",
            "Type: 'texts()' or 'sents()' to list the materials.\n",
            "text1: Moby Dick by Herman Melville 1851\n",
            "text2: Sense and Sensibility by Jane Austen 1811\n",
            "text3: The Book of Genesis\n",
            "text4: Inaugural Address Corpus\n",
            "text5: Chat Corpus\n",
            "text6: Monty Python and the Holy Grail\n",
            "text7: Wall Street Journal\n",
            "text8: Personals Corpus\n",
            "text9: The Man Who Was Thursday by G . K . Chesterton 1908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "문장 토큰화"
      ],
      "metadata": {
        "id": "xwPWobfR7UY9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SEiUYCpF5tYz",
        "outputId": "da3dc017-0956-445c-bed3-d30fbffc0552"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "['The Matrix is everywhere its all around us, here even in this room.', 'You can see it out your window or on your television.', 'You feel it when you go to work, or go to church or pay your taxes.']\n"
          ]
        }
      ],
      "source": [
        "from nltk import sent_tokenize\n",
        "text_sample = 'The Matrix is everywhere its all around us, here even in this room.  \\\n",
        "              You can see it out your window or on your television. \\\n",
        "               You feel it when you go to work, or go to church or pay your taxes.'\n",
        "sentences = sent_tokenize(text=text_sample)\n",
        "print(type(sentences) , len(sentences))\n",
        "print(sentences)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "단어 토큰화"
      ],
      "metadata": {
        "id": "_LqYnsbD7WbT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "word = word_tokenize(sentence)\n",
        "print(type(word) , len(word))\n",
        "print(word)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tPC2w0to52AF",
        "outputId": "7bfda8cd-bb0c-4fa5-8d53-34a18c3f8cff"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 15\n",
            "['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize , sent_tokenize\n",
        "def tokenize_text(text):\n",
        "  sentences = sent_tokenize(text)\n",
        "  word_tokens = [word_tokenize(sentence) for sentence in sentences]\n",
        "  return word_tokens\n",
        "\n",
        "word_tokens = tokenize_text(text_sample)\n",
        "print(type(word_tokens) , len(word_tokens))\n",
        "print(word_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYJyeMpw7elQ",
        "outputId": "320392d9-c638-4a44-fea8-638f1ada4e5b"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'list'> 3\n",
            "[['The', 'Matrix', 'is', 'everywhere', 'its', 'all', 'around', 'us', ',', 'here', 'even', 'in', 'this', 'room', '.'], ['You', 'can', 'see', 'it', 'out', 'your', 'window', 'or', 'on', 'your', 'television', '.'], ['You', 'feel', 'it', 'when', 'you', 'go', 'to', 'work', ',', 'or', 'go', 'to', 'church', 'or', 'pay', 'your', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "n-gram"
      ],
      "metadata": {
        "id": "vmjN1JDN8FcJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import ngrams\n",
        "sentence = 'The Matrix is everywhere its all around us, here even in this room.'\n",
        "words = word_tokenize(sentence)\n",
        "all_ngrams = ngrams(words , 3)\n",
        "print(all_ngrams)\n",
        "ngrams = [ngram for ngram in all_ngrams]\n",
        "print(ngrams)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qC3X9a9Q78Kr",
        "outputId": "2aea7d14-5f9d-4993-877e-9775360298d6"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<zip object at 0x7eee05d278c0>\n",
            "[('The', 'Matrix', 'is'), ('Matrix', 'is', 'everywhere'), ('is', 'everywhere', 'its'), ('everywhere', 'its', 'all'), ('its', 'all', 'around'), ('all', 'around', 'us'), ('around', 'us', ','), ('us', ',', 'here'), (',', 'here', 'even'), ('here', 'even', 'in'), ('even', 'in', 'this'), ('in', 'this', 'room'), ('this', 'room', '.')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stopwords Remove"
      ],
      "metadata": {
        "id": "wcRyRJgd8boE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('stopwords')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLiFuwZj8Uie",
        "outputId": "52fa7dfa-7ddf-4c44-93b3-6d3b56142f57"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"영어 stop words 개수 :\", len(nltk.corpus.stopwords.words('english')))\n",
        "print(nltk.corpus.stopwords.words('english')[:10])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3yapliy8myQ",
        "outputId": "7f4f0df6-7ee2-4235-f49a-77159b77da5f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "영어 stop words 개수 : 179\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = nltk.corpus.stopwords.words('english')\n",
        "all_tokens = []\n",
        "for sentence in word_tokens:\n",
        "  filtered_words = []\n",
        "  for word in sentence:\n",
        "    word = word.lower()\n",
        "    if word not in stopwords:\n",
        "      filtered_words.append(word)\n",
        "  all_tokens.append(filtered_words)\n",
        "print(all_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5mCZf6nE8xDZ",
        "outputId": "524af7db-4f1e-4994-e375-47de1f612e1f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['matrix', 'everywhere', 'around', 'us', ',', 'even', 'room', '.'], ['see', 'window', 'television', '.'], ['feel', 'go', 'work', ',', 'go', 'church', 'pay', 'taxes', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming , Lemmatization"
      ],
      "metadata": {
        "id": "MgfSREtw9gji"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import LancasterStemmer\n",
        "stemmer = LancasterStemmer()\n",
        "print(stemmer.stem('working') , stemmer.stem('works') , stemmer.stem('worked'))\n",
        "print(stemmer.stem('amusing') , stemmer.stem('amuseer'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghop-JO49Wfi",
        "outputId": "01f58e1d-bfba-4e6d-aa53-08d8b0a9062d"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "work work work\n",
            "amus amus\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        "# nltk.download('wordnet')\n",
        "lemma = WordNetLemmatizer()\n",
        "print(lemma.lemmatize('amusing','v') , lemma.lemmatize('amuses','v'))\n",
        "print(lemma.lemmatize('happier' , 'a') , lemma.lemmatize('happiest' , 'a'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jy5if9dp9c6u",
        "outputId": "9870f6f8-bad9-44bd-86e5-f06c206e683e"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "amuse amuse\n",
            "happy happy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cA5LCa7C985k"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}